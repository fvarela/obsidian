
Many ways to authenticate:
	* Access Data Lake using AAD Credential Pass-through
	* Entering the Secrets
		* Where - Cluster Scoped Authentication
		* Where - On the Notebooks
		* How - Access Data Lake using Service Principal	
		* How - Access Data Lake using Access Keys
		* How - Access Data Lake using SAS Token
		* Security Note - Secure your secrets!
		

## Access Data Lake using Credential Pass-Through

This allows to access Azure Resources without writing credentials in your code. This is the only authentication that is defined for individual users. All the others provide access to whoever has the keys. **This is the best option** but you need to be able to assign roles in order to use it. A contributor cannot do that

1. Go to the cluster configuration and check the box "Azure Data Lake Storage credential passthrough" (I can't do that)
2. Go to the storage account/ Access Control (IAM)
	1. Add the role "Blob Storage Contributor" to yourself

![[Pasted image 20240126092522.png]]


Then you can just access the resource without using keys:
```python
diplay(dbutils.fs.ls("abfss://demo@formula1dl.dfs.core.windows.net"))
```



## Entering the Secrets
### Where - Cluster Scoped  Authentication


Set the configurations you need in the cluster as key value pairs:
![[Pasted image 20240126091746.png]]

You can also access the secret using the dbutil like this:


That would be equivalent as access your secret from the notbook with:
``` 
formula1dl_access_token = dbutils.secrets.get(scope="vps-vida-keys-scope", key="formula1dl-access-key")

spark.conf.set("fs.azure.account.key.formula1dlfvv.dfs.core.windows.net",

               formula1dl_access_token)
```

### Where - Notebook Scoped Authentication
Just write the same configurations but in the notebooks.


### How - Using Service Principal (Microsoft Entra ID)

Recommended. Better security
1. Register Azure AD (Microsoft Entra ID) Application / Service Principal and generate a secret for the Application

	Go to Azure Portal / Azure Entra Id / App Registrations
	Click on New Registration (this is called Service Principal)
		Call it whatever you want
		Click on Register
		Copy keys:
			Application ->(clientId)
			Directory -> (tenantId)
		Click on Certificates and Secrets
			Generate a client Secret
			Copy the Secret Value (not the id). ->(secretValue)
			

2. Set Spark Config with App/ Client Id, Directory/ Tenant Id & Secret

```python
service_credential = dbutils.secrets.get(scope="<secret-scope>",key="<service-credential-key>")

spark.conf.set(
	"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", 
	"OAuth")
spark.conf.set(
	"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net",
	"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
	
spark.conf.set(
	"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", 
	"<clientId>")
	
spark.conf.set(
	"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", 
	secretValue)
	
spark.conf.set(
	"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", 
	f"https://login.microsoftonline.com/{<tenantId}/oauth2/token")
```

4. Assign Role __Storage Blob Data Contributor__ to the Data Lake
Go to the storage account / Access control (IAM) / Add Role Assignment
Add the role of 'Storage Blob Data Contributor' to the Service Principal you created before.

### How - Using the Access Keys

- How to:
	1. Set up the configuration
	```python
	spark.conf.set(
	"fs.azure.account.key.<storage-account>.dfs.core.windows.net",
	"<azure_storage_access_key>"
	)
	```
	2. Then you can use the abfs[s] driver to access the files:
	
```python
dbutils.fs.ls("abfss://<container_name>@<storage_name>.dfs.core.windows.net/")
```

Problem:  The access key give access to the whole storage account. In order to provide access to what is only needed for the project you can use SAS:

### How - Using SAS Token

- How to:
	1.  Set up the configuration for the SAS token:
```python
spark.conf.set(
	"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net",
	"SAS")
spark.conf.set(
	"fs.azure.sas.token.provider.type.<storage_account>.dfs.core.windows.net",
	"org.apache.hadoop.fs.azurefs.sas.FixedSASTokenProvider")
spark.conf.set(
	"fs.azure.sas.fixed.token.<storage_account>.dfs.core.windows.net",
	"<token>")
```

### Security Note - Secure your secrets! 
You need to create:
	Azure key-vault
	Databricks backed Secret Scope
And link the two together

* Use Databricks backed Secret Scope
	* To create it apppend `#secrets/createScope` to the Databricks URL:
	`https://adb-2323455902314664.4.azuredatabricks.net/?o=2323455902314664#secrets/createScope`
	* There you can add the information of your Azure key-vault:
		* Retrieved the needed info from Azure key-vault properties
			* Vault URI -> DNS Name
			* Resource ID



* Accessing the secrets with dbutils:

	```python
	dbutils.secrets.help()
	dbutils.secrets.listScopes()
	dbutils.secrets.get(scope='formula1-scope', key='formula1-account-key') 
	```




