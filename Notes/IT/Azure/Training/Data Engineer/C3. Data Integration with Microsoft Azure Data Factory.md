Integrate data with Azure Data Factory or Azure Synapse Pipeline
Understand Azure Data Factory
	Understand Azure Data Factory
		ADF - Azure Data Factory
		Orchestrates the movement and transformation of data between various data stores and compute resources
		Pipelines: Data Driven workflows
		Build ETL processes with:
			Data Flows
			Or Computer services:
				Azure HD insight
				Hadoop
				Azure Data Bricks
				Azure Synapse Analytics
		The same 'pipeline' functionality can be obtained in Azure Syanpse Analytics
			Integrate data pipelines between 
				SQL pools
				Spark pools
				SQL serverless
		Orchestration
			Azure Data Factory orchestrate services. Ej:
				DataBricks to to the transformation query
	Data Integration Patterns
		Azure Data Factory can implement ETL
			Extraction
				Engineers define
					Data source
					Data 
						SQL Query
						Set of files
						Blobs...
			Transformation
			Load
				Define the destination
				Many Azure destinations accesp data formatted as Json, for instance.
				You might need to code to interact with APIs
		Data Factory has built in support for Azure Functions
		To test the ETL use a development or test environment
		ADF provides nearly 100 enterprise connectors for both code free and code based users
		ELT is also supported.
			ELT can store the data in the original format
			Define de data structure during the transformation phase
		ELTL. Extract Load, Transform and Load
		Two main Data Integration Patterns supported:
			Modern Data Warehouse workloads
				Centralized data store
			Advanced Analytical workloads			
	Explain the data factory process			
			Pipelines typically perform the following four steps:
				Connect and collect
					Connect all the data sources together such as
						DBs
						FTP
						File shares
					Ingest the data as needed to a centralized location
				Transform and enrich
					Using tools such as:
						DataBricks
						Machine Learning
				Publish
					After the raw data has been pulished to ready to consume data you can publish the data into:
						Azure DataWarehouse
						Azure SQL
						Azure Cosmos DB
				Monitor
					ADF has built in support to check the pipeline.
	Azure Data Factory Components
		Linked services
			These are connection strings
			With these you connect the data factory to external resources.
				Azure data lake store
				Azure Databricks
		Datasets
			Points or references the data to the linked services
		Activities
			Activities allows you to perform an action on the data.
			You can have one activity but tipically will have multiple activities
		Pipeline
			Activities grouped together
			Allows you to manage the activities as a set instead of individually.
			You deploy and schedule the pipeline
		Integration runtime
			Compute infrastructure used by ADF
			It provides the compute resources as close as possible to the data
		Control Flow
			Control the activities within the solution
	Undestand Azure Data Factory Components
		An Azure subscription can have many Data Factories
		A Data Factory is composed of four components:
			Linked Services
				Connects to data sources
				Fir up compute services on demand
				With this ADF knows what Data sourcers is going to use to the creation of the Dataset.
			Datasets 
				Data Structures within the data store that are being references by the linked services.
			Activities
				Control Flow is an orchestraction or pipeline activities
			Pipeline
		Parameters
			Key-value pairs of read only configuration
			Are passed during execution of the run context
			Activities within the pipeline consume the parameters
		Integration runtime
			Enables to bridge betwwen
				Activity
				Linked services
			Referenced by the linked service
			Provides the compute environment wherer the activity runs or get dispatched from.
			Three types of integration runtimes:
				Azyure
				Self hosted
				Azure-SSIS
		Once all the work is complete you can use the data factory to publish the final data set to another liked service that can be then consumed by technologies such as power bi.
	Azure Data Factory Security
		To create a data factory you have to be:
			Owner or
			Contributor
			Administrator or the Subscription
		To create and manage Data Factory objects (Datasets, piplines, triggers, integration runtimes...):
			To create child resources:
				You must be Data Factory contributor
			To create and manage resources with PowerShell or the SDK:
				Contributor role	
		About Data Factory Contributor Role:
			You can create, edit and delete data factories and child resources.
			Resource manager deployment.
				This is the deployment method used in ADF in the Azure Portal
			Manage App insights alerts in ADF
			Resource manager template
			Support tickets
	Set up and manage Azure Data Factory			
		Lesson Introduction:
			You need to provide the followin information:
				Name
					The name of the Azure Data Factory instance
				Subscription
					The subscription in which the ADF instance is created
				Resource group
					The resource group where the ADF instance will reside
				Version
					Select V2 for the latest features
				Location
					The datacenter location
		Example - Create linked services
			Can be defined using the Copy Data Activity in the ADF designer
			Can be created independently to point to:
				A data store
				A computer service
			Can be created programmatically using the rest api or SDK by passing a json object with this info:
				name
				type typeProperties
				connectVia
			Examples of linked services:
				Azure SQL Database
				Azure Blob Storage
		Example - Create Datasets
			A Dataset is a 'name' that points the data you want to use.
			It can point to a folder, table, etc
			Can be defined as an object within the Copy Data Activity
			Can be defined as a separate object
			Can be defines as a JSON object
				name
				type
					Azure blob
					Azure SQL etc
				structure
					Schema of the dataset
				typeProperties
					Different for each data type:
						Azure blob
						Azure SQL table...
		Example - Create Data Factory activities and pipelines
			Activities: Actions that will be performed on the data
			Three categories:
				Data movement activities
					Move the data from one store to another
					Define it:
						Copy Activity
						JSON
				Data transformation activities
					Choices:
						Natively in the ADF using the Mapping Data Flow
						Call a compute resource:
							Azure Databricks
							Azure Batch
							SQL Database
							Azure Synapse Analytics
							Machine Learning Services
							Azure Virtual Machines
							HDInsight
							SQL Server Integration Services
						JSON	
				Control activities
		Manage integration runtimes
			Activity - Action to be performed
			Linked service - Target data store or compute service
			Integration runtime
				Provides infrastructure for activities and linked services where the activity runs
				The activity runs in the region rearest to the target data store while meeting security and compliance needs.
				When an Azure Data Factory instance is created a default integration runtime environment is created that supports operations. This can be viewed when the integration runtime is set to autoresolve.
				Three types of integration runtimes:
					Azure integration Runtime (Azure IR)
						Supports:
							Public network support with:
								Data flow
								Data movement
								Activity dispatch
					Self hosted integration service
						Supports:
							Public and private network support with
								Data movement
								Activity dispatch
					Azure SQL Server Integration service (Azure SSIS IR)
						Supports:
							Public and private network support with SSIS package execution
				You can define the type in 'connect via' property. Default is 'autoresolve'.
				Guidelines to select the right runtime:
					Copy
					Lookup and get metadata
						Executed on the datastore linked service.
					Transformation
						Each one has a target compute linked service which points to a IR. There is where the transformation is executed
					Datafrlow
				You can install a self-hosted IR inside your virtual private network
					It only makes outbound HTTP-based connections to open internet
					If you copy files between Cloud and the private network. The copy activity is executed on that self-hosted integration runtime.
					You can also run SSIS packages by installing an SSIS integration runtime.
Ingest Source Data				
	List the data factory ingestion methods
		Three methods:
			Copy Activity
				If you data don't require any transformation during the extraction
				Support for over 100 connectors
			Compute resources
				To process the data by a data platform service
				Example:
					Spark pool in Azure Synapse Analytics
				Available services (examples):
					Azure Machine Learning,
					Azure SQL
					Azure Databricks
					Azure Functions
			SSIS packages (SQL Server Integration Services)
				To use existing SSIS workload withl little to no change using SSIS tools like:
					SQL Server Datat Tools
					SQL Server Management Studio
	Describe Data Factory Connectors
		Connectors are Azure Data Factory objects that enable your linked services and datasets to connect to a wide variety of data sources and syncs:
			Azure resources
			Third party:
				Amazon S3
				Google Cloud etc
		They work with the ADF activities:
			Copy
			Data Flow
			Look up
			Get Metadata
			Delete
		Supported formats:
			Avro
			Binary
			Delimited
			Json
			ORC
			Parquet
	Understanding data ingestion security considerations:
			There is a need for a holistic security approach
				Network
					Vitual Networks
						Allows secure communications between Azure services
						Allows secure communication with on premisses networks
						* If your Azure Data Factory wants to get data from a Virtual Network hosted inside Azure you need no install a Self Hosted Integration Runtime on the server inside the Virtual Network. To restrict access set up a NSG (Network Security Group) Inbound Rule.
						* If using SSIS Azure Integration Runtime you are giving the option to join a Virtual Network. That option enable Data Factory to create network resources, for example a NSG is automatically created and port 3389 is open.
					Services
						You  can use them to detect and prevent intrussions.
						Various services are available:
							Deny comms with known IP addresses by enabling DDoS protection
							Azure Firewall with threat intelligence can be used to control network access.
							Azure ExpressRoute
								Intrussion detection
								Prevention based on payload inspecctión
					Security rules
						You can simplify the security rules using Network Service Tags
							Service Tags allows you to group together IP addresses
							Create network security rules based on Service Tags.
					Identity and access
						Administrative accounts
							These accounts should be dedicated and checked regularly so that they are not compromised
							To create Data Factory instances the user hast to be:
								Contributor
								Owner
								Administrator
						Active directory
							Register service principals in Active Directory to take advantage of token management so that your Azure Data Factory service is authenticated across Azure resources.
					Data protecion
						Pay attention if working with sensitive data:
							Role Based Access Control
							Sensitive Data
								Considerations:
									Data Stores - Have a list of the data stores that contain sensitive information
									Isolate Systems
									Monitor and Block
									Encrypt in transit
									Encrypt at rest
					Logging and monitoring
						Azure Monitor centralize the storage of ingestion logs that are generated by ADF
						Azure Log Analytics to query the logs.
						Azure Storage Account to store the logs long term
						Enable NSG (Network Security Group) logs and send them to a Storage Account
						To enable logging -> Azure Data Factory diagnostic settings
							Data is retained for 45 days
Perform code-free transoformation at scale with Azure Data Factory or Azure Synapse Pipeline
	Lesson introduction
		Azure Data Factory transformation methods
			Three methods:
				Mapping data flow
					No code solution
					Visually designed
					Follows an ELT approach
					The flows are executed as Activities
					Available transformations:
						Aggregate
						Conditional Split
						Exists
					Performs:
						Data cleansing
						Transformation
						Aggregations
					The following datasets can be used in a source transformation:
						Azure Blob Storage (Json, Avro, text, parquet)
						Azure Data Lake Storage gen1
						Azure Data Lake Storage gen2
						Azre Synapse Analytics
						Azure Cosmos DB
				Executing SSIS packages	. 
					You need an SSIS Integration Runtime
				You can transform data using a compute resource such as:
						Azure Databricks
						Azure Machine Learning
						Azure HDInsights
		Explain Azure Data Factory transformation methods
			Using mapping data flows:
				Create the flows visually
				Flows are executed on Apache Spark clusters
				You can review the status of the transformations visually
				Can use other data platform services (HDInsights, Synapse Analytics, Spark, etc)
			Using SSIS (SQL Server Integration Services)	
				With this you cand deploy and manage existing SSIS packages
				Allows you to use tools such as:
					SQL Server Data Tools (SSDT)
					SQL Server Management Studio (SSMS)
		Azure Data Factory Transformation types
			For Mapping data flows :
				Schema modifier transformations
				Row modifier transformations
				Multiple input/outputs transformations
		Debug mapping data flow
			By clicking the debugger ADF provision a Spark Cluster.
			You are prompted to select a integration runtime
			If you select AutoresolveIntegrationRuntime it will provision 8 cores for 60 minutes.
			Data Preview Tab allows you to check the process at each step of the pipeline
		Use Data Factory Wrangling data
			Wrangling Data Flow is a data flow object that can be added to the canvas designer as an activity to perform code free data preparation.
			Good if you don't know technologies such as Spark or SQL server or languages such as Python and T-SQL	
			Wrangling Data Flow uses a grid type interface that looks like Excel or PowerBi
	Populate slowly changing dimesions in Azure Synapse Analytics pipelines
		Describe slowly changing dimensions
			SCD (Slowly changing dimensions) are tables in a dimensional model that handle changes to dimension values over time.
			Common desigh approach:
				Rapidly changing attributes in a fact table
				Slowly changing dimensions in dimensions table
			The table design varies depending on if you choose to update values without much history or to tract each version of history
		Choose between slowly changing dimension types:
			Star schema design theory refers to common SCD types
			Types
				Type1:
					Just update the fields
					Always reflects the latests values
					When changes are detected the dimension table data is overwritten
				Type2:
					- Supports versioning of dimension members
					- If the data source don't accept version you have to use an intermediate system like a data warehouse
					- The dimesion table must use a surrogate key
					- Includes columns that define the time range validity of the version
					- Can include a flag column like 'isCurrent'
				Type3:
					Support two versions of a dimesion member as separate columns
				Type6:
					- Combines type1, type2 and type3
		Design and implement a Type 1 slowly changing dimension with mapping data flows		
Orchestrate data movement and transformation in Azure Data Factory or Azure Synapse Pipeline
	Understand data factory control flow
		Control Flow
			In Azure Data Factory control flow is an orchestration of pipeline activities:
				Chaining activities
				Branching
				Defining parameters at the pipeline level
				Passing arguments while invoking the pipeline
			Control Flow Activities
				Chain activities within a pipeline
				Use 'dependsOn' property on an activity to chain it with an upstream activity
				Branch activities with the 'if condition'
					If the condition is met a set of activities is executed
					If not a different activity is executed
				Pipelines can be executed:
					On demand
					Time based
					Event based
				Three types of triggers:
					Schedule trigger - World clock schedule
					Tumbling window trigger - Periodic interval while mantaining state
					Event base trigger - Responds to an event
				'Execute pipeline' activity allows you to invoke a pipeline from a different pipeline
				Delta Loads in ETL patterns will only load data that has changes since the previous iteration
	Work with data factory pipelines
		Pipeline is a logical grouping of activities
		Activities are actions you perform on your data
		Activities have one or more input datasets and have one or more output datasets
		Activities. Three categories:
			Data movement
			Data transformation
			Control activities
				They use dependency conditions:
					Succeeded
					Failed
					Skipped
					Completed
	Debug Data Factory Pipelines
		By using the pipeline Canvas you can test your activities and pipelines by using the debug capability.
		There is no need to publish changes to debug it
		Monitor tab:
			You can see all the active debug runs
			It keeps Debug run history for 15 days
	Add parameters to data factory components
SQL Server Integration Services
	Describe SQL Server integration services
		SSIS (SQL Server Integration Serviced) can be integrated into the cloud easily
		SSIL is a platform to build ETL solutions
		Can be used to
			Data Integration pipelines
			Data migration pipelines
		SSIS manages control flows
		Control Flows are held in packages
		Control Flows contian one or more tasks
		An SSIS solution contains one or more SSIS projects
		An SSIS project contains one or more SSIS packages
		The project is the unit of deployment for SSIS solutions
		The SSIS projects were introduced in SQL Server 2012
		In previous releases deployment was managed at the package level.
	Understand the Azure SSIS Integration Runtime
		An Azure SSIS Integration Runtime can execute SSIS packages
		The location of the SSIS IR should be the same as the location of the Azure SQL Database or Azure SQL Database Managed Instance.
		To use SSIS IR you need a SSIS Catalog or SSIS DB deployed
		SSIS IR options:
			Number of nodes in cluster
			Existing instance of Azure SQL database to host the SSIS catalog
				OR
			SSIS DB and the service tier for the database
			Maximum parallel executions per node
Use Azure Data Factory to build your data integration solutions
	Lesson Introduction
	Undestand the language support in Azure Data Factory
	Manage source control of Azure Data Factory solutions
		By default the user interface experience is UX
			This has limitations:
			Does not include a repository for storing the JSON entities for changes
			The Data Factory service is not optimized for collaboration and version control
			Publish only via the publish button
		Can use git repository
			If git is configured altering the data factory service is not allowed.
			Advantages
				Source control
				Check changes before publishing
				Better performance. Resources are downloaded via git
				Partial saves
				Better CI/CD
			Number of allowed parameters on the template: 256
			Permission to update the ADF: Data Factory Contributor
			Setup GIT from any of these options:
				ADF Authoring Hub
				ADF Management hub
				During factory creation
				ADF Home Page
			You can publish to the Data Factory service from your collaboration branch only
		Alerts:
			Add criteria configures how an alert is fired
Ingest Data from Azure Data Share into Azure Data Factory Pipelines
	Module introduction:
		Data consumers have an option to automatically get refreshed datat on a shared dataset. this is called: Snapshot
	Share and receive data using Azure data Share and transform data using Azure Data Factory
	In-Place Sharing. With this option you enable access to data at source.
	It is possible to add or remove datasets within Azure Data Share after it has been created.
	The following data stores provide incremental snapshots:
		Azure Data Lake Storage Gen1
		Azure Data Lake Storage Gen2
		Azure Blob Storage
Join and transform shared and owned data
	Mapping data flow
		Follows ELT approach
		To combine tables you can use: A mapping datat flow
		Possible joins:
			Left outer
			Inner join
			Right outer
			Full outer
	Sink datasets int Azure Synapse Analytics
		Data flow requires at least one sink.
		Multiple sinks are also possible
		Each sink is associated with one Azure Data Factory dataset or linked service
		Azure synapse analytics only supports the datasets option in its source transformation
	Publish a pipeline run in Azure Data Factory
		If you use the Publish all option, Azure Data Factory first runs a validation check
		In the monitor tab you have run information for the last 45 days.
		If you trigger manually the data flow it will start a just-in-time Spark cluster. It might take 5-7 minutes since the Spark cluster has to be activated.