## C1,C2,C3

Which term is related to the cost of on-premisses servers:
	TCO - Total Costs of ownership

Which Azure data platform is commonly used to process data in an ELT Framework:
	Azure Data Factory

Moving resources such as servers and services from an on-premises environment to a cloud-based solution will have a benefit on which of the following.
	Both capital and operational expenditure

Which of the following data processing frameworks are used by data engineers to ingest data from an on-premises database to an on-premises data warehouse?
	ELT is a typical process for ingesting data from an on-premises database into the cloud.

Unstructured data differs from Structured Data in many features. Which of the following are features of Unstructured data? Select all options that apply.
	X Commonly stored in data lakes
	Schema-on-write
	Commonly stored in data warehouses
	Predefined format
	X Schema-on-read.
	Native format

Data Engineers may sometimes perform ETL process when processing data. The extract may come from many sources including databases, files, and streams. These data sources can have unique data formats including which of the following? Choose all that apply.
	X Semi-structured
	X Unstructured
	Partially structured
	X Structured

Which data processing framework will a data engineer use to ingest data onto cloud data platforms in Azure?
	X Extract, load, and transform (ELT)
	Online transaction processing (OLTP)
	Extract, transform, and load (ETL)

Which Azure service is the best choice to store documentation about a data source?
	Azure Data Factory
	Azure Data Lake Storage
	X Azure Data Catalog

Azure Synapse Pipelines leverages the capabilities of Azure Data Factory and is a cloud-based solution for which of the following types of data ingestion and transformation?
	ELT
	X ETL

Data Engineers configure ingestion components of Azure Stream Analytics by configuring data inputs from sources including Azure Event Hubs, Azure IoT Hub, or Azure Blob storage. Azure Event Hub provides which of the following features. Select all that apply!
	X Big data streaming service
	Azure Storage
	X Partitioned consumer model

Azure Storage Analytics service can be used to audit which of the following? Select all options that apply.
	X Resources that have been accessed.
	Success of operations
	Azure AD permission changes
	X Authentication mechanism

Which of the following is a key characteristic of Azure Government?
	Azure Government has a portal from which you can manage your resources.
	Azure Government has a marketplace from which you can deploy pre-built images from Microsoft and partners.
	X Azure Government is a physically separate instance of Azure.

Azure Defender for Storage is available in which of the following environments? Select all options that apply.
	X US government clouds
	Sovereign cloud regions.
	X All public clouds
	Azure Government

Blobs are usually not appropriate for structured data that will be queried frequently. Which of the following is a characteristic? of blobs?
	Indexing features
	X High latency
	Restrictions on data types

You are working on a new project and need to create storage accounts and blob containers for your application. What is the best way to do this?
	X Create Azure storage accounts before deploying your app and then create containers in your application as needed.
	Create Azure storage accounts in your application as needed and create the containers before deploying your app.
	Create both the Azure storage accounts and containers before deploying your application.

What is the maximum size of a block blob storing text or binary files?
	X 5 TB
	500 GB
	5 GB

Azure Storage provides a REST API to work with the containers and data stored in each storage account.What would the following HTTP command return? GET https://[url-for-service-account]/?comp=list&include=metadata
	X A list of all Blobs
	A List of all Files
	A list of all tables

Azure Storage accounts can create authorized apps in Active Directory to control access to data in which of the following storage services? Select all options that apply.
	XAzure Queues
	Azure Tables
	XAzure Blobs
	Azure Files

Azure Storage offers several types of storage accounts. What is the recommended storage type for block blobs and append blobs?
	Standard General purpose v2
	X Premium Storage

Azure File Storage exposes file shares using what protocol?
	HTTPS
	Server Message Block 2.0
	X Server Message Block 3.0

Microsoft Azure provides a variety of data platform services that enables you to perform different types of analytics. Predictive analytics can be implemented through which of the following features?Select all options that apply
	X HDInsight
	Azure Data Lake Storage Gen2
	X Machine Learning Services
	X Azure Databricks

You are creating a new Azure Data Factory instance. The instance name must be unique within which of the following?
	X Globally within Azure
	The region
	The Azure Subscription
	The Resource Group

When working with Azure Data factory, how many linked services are required to copy data from Blob storage to a SQL database?
	X 2
	3
	1
	0

When graphically authoring ADF solutions, you can use the control flow within the design to orchestrate which of the following pipeline activities?
	X WebActivity
	X ForEach Activity
	Parameters Activity
	X Execute Pipeline Activity

What are the three categories of activities within Azure Data Factory that define the actions to be performed on the data?
	XData Transformation
	XControl
	Linked Service
	Data Movement

You are moving data from an Azure Data Lake Gen2 store to Azure Synapse Analytics. Which Azure Data Factory integration runtime would be used in a data copy activity?
	Self-hosted
	X Azure
	Azure-SSIS

$ Data Factory offers three types of Integration Runtime.-   Azure-   Self-hosted-   Azure-SSIS. Which of these provides support for both Public Networks and Private Networks?
	Azure only
	Self Hosted and Azure-SSIS
	? Azure-SSIS only
	Azure and Azure-SSIS
	Azure and Self Hosted
	Self Hosted only

Connectors are Azure Data Factory objects that enable your Linked Services and Datasets to connect to a wide variety of data sources and sinks. Which of the following are supported file formats?Select all options that apply.
	X Parquet format
	X Binary format
	YAML Format
	X ORC format

In Azure Data Factory when creating a new Copy Activity, which of the following steps do you carry out?
	Click on Manage, and add a new copy activity
	X Open the authoring canvas, create a new Pipeline, or use an existing pipeline and then add a new copy activity
	Click on Author, and create a new copy activity
	Click on Monitor and add a new pipeline and copy activity

You want to ingest data from a SQL Server database hosted on an on-premises Windows Server. What integration runtime is required for Azure Data Factory to ingest data from the on-premises server?
	X Self-Hosted Integration Runtime
	Azure-SSIS Integration Runtime
	Azure Integration Runtime

Azure Data Factory can call on compute resources to process data by a specific data platform service that may be better suited to the job. Which compute resource can carry out the following activities, Spark, MapReduce and Hadoop Streaming activities?
	Azure Machine Learning
	Azure Batch
	X On-demand HDInsight cluster
	Azure Databricks

$ What are the benefits of Azure-SSIS Integration Runtime? Select all options that apply.
	X Deploy and manage existing SSIS packages
	X Natively execute existing SSIS packages
	Process Azure Databricks Activities
	Create On-demand HDInsight clusters

In Azure Data factory integration runtime defines which of the following?
	X The bridge between the activity and linked services
	The action to be performed
	The Target Data Store or Compute service

In the Azure Data Factory Authoring Tool, where would you find the Copy data activity?
	Batch Service
	X Move & Transform
	Databricks

You can use which of the following tools to deploy and manage existing SSIS packages using Azure Data Factory? Select all options that apply.
	Azure Data Factory Pipelines
	Azure Copy Activity
	X SQL Server Data Tools (SSDT)
	X SQL Server Management Studio (SSMS)

When configuring Azure Data Factory integration runtimes you can deny communication with known IP addresses. Which of the following must you enable to do so?
	X Azure Security Center Integrated Threat Intelligence
	X Azure Firewall with Threat Intelligence
	X Distributed Denial of Service (DDoS)
	Azure Identity and Access management (IAM)

Which of the following transformations are directly available in the Mapping Data Flows activity?Select all options that apply.
	X Aggregate
	X Exists
	Insert
	X Conditional Split

Mapping Data Flow follows an extract, load, transform (ELT) approach and works with staging datasets that are in Azure. Which of the following datasets can be used in a source transformation?Select all options that apply.
	X Azure Synapse Analytics
	Microsoft SQL Server
	X Azure CosmosDB
	X Azure Blob Storage (JSON, Avro, Text, Parquet)

Mapping Data Flows support Debug so that you can interactively watch how the data transformations are executing. When enabling Debug you are be prompted to select the Integration runtime to use. If you select AutoResolveIntegrationRuntime a cluster will be made available automatically. How many Cores will be made available in the cluster and what will the time to live be in minutes?
	4 Cores and 60 Minutes
	X 8 cores and 60 minutes
	4 Cores and 30 Minutes
	8 Cores and 30 Minutes

When working with Azure Data Factory and wrangling data flows. You add a Source dataset for your wrangling data flow and select a sink dataset. If you use an Azure Data Lake Storage Gen2 Connector what type of authentication is supported in this instance? Select all options that apply.
	X Service Principal
	SQL authentication
	Anonymous Access
	X Account Key
$ You want to load a dimension table in Azure Synapse from source data in your Azure Synapse database using mapping data flows. Which of the following options would you choose in Synapse Studio Hub to create a Type 1 SCD in a Mapping Data Flow pipeline activity?
	Integrate
	Data
	Manage
	X Develop

Azure Data Factory can also call on compute resources to transform data. Which Compute environment can be used with Notebook, Jar and Python Activities?
	Azure Machine Learning Studio Machine
	Azure SQL, Azure SQL Data Warehouse, SQL Server
	Azure Data Lake Analytics
	X Azure Databricks

You are currently working on a Mapping Data Flow transformation for a Movie Database. You are interested in how a movie ranks within its year for its specific genre. Which of the following transformations should you use to generate the required transformation?
	Aggregate Transformation
	X Window Transformation
	Alter Row Transformation
	A Derive Transformation

When creating transformations in Azure Data factory which of the following uses a grid type interface for basic data preparation similar to Excel, known as an Online Mashup Editor, while also allowing complex data preparation using formulas?
	Azure-SSIS Integration Runtime
	Mapping Data Flow
	X Wrangling Data Flow

Which transformation is used to load data into a data store or compute resource?
	Surrogate key
	Source
	X Sink
	Window

Which of the following transformations are directly available in the Mapping Data Flows activity?Select all options that apply.
	X Exists
	Insert
	X Aggregate
	X Conditional Split

In Azure Data Factory to get a view of the history of debug runs, or see all the active debug runs, you can navigate to the monitor tab. How many days does Azure Data Factory service keep Debug run history for?
	30 Days
	365 Days
	X 15 days
	5 Days

In Azure Data Factory activities in which repetition occurs in a pipeline enabling you to iterate over a collection is referred to as which of the following?
	Pipeline
	X Looping containers
	Branching activity
	Custom State passing

One of the features of Azure Pipelines is the ability to create Continuous Integration and Continuous Delivery (CI/CD) to persistently build and test the code and finally ship a high-performing, high-quality product. Which of the following definitions are correct? Select all options that apply.
	Continuous Integration helps in deploying the integrated and built code into different delivery stages as new versions of the product.
	X Continuous Delivery helps in deploying the integrated and built code into different delivery stages as new versions of the product.
	Continuous Delivery produces deployable code, infrastructure, and other artifacts to be deployed
	X Continuous Integration produces deployable code, infrastructure, and other artifacts to be deployed.

In Azure Data Factory which of the following is a supported connector for built-in parameterization for linked services?
	Azure Data Lake Storage Gen2
	Azure Key Vault
	X Azure Synapse Analytics

When creating a new SSIS Integration Runtime which of the following pieces of information are required on the General setting page? Select all options that apply.
	X Location
	X Node Number
	X Name
	Description
	XNode Size

$ When you migrate your database workloads from SQL Server on premises to Azure SQL database services, you may have to migrate SSIS packages as well. The first step required is to perform an assessment of you current SSIS packages to make sure that they are compatible in Azure. Which of the following tools would be most appropriate to check for compatibility issues?
	SQL Server Management Studio (SSMS)
	SQL Server Data Tools (SSDT)
	X Data Migration Assistant (DMA)

When you migrate your database workloads from SQL Server on premises to Azure SQL database services, you may have to migrate SSIS packages as well. The first step required is to perform an assessment of your current SSIS packages to make sure that they are compatible in Azure. Which of the following tools would be most appropriate to check for compatibility issues?
	SQL Server Data Tools (SSDT)
	X Data Migration Assistant (DMA)
	SQL Server Management Studio (SSMS)


Which tool is used to create and deploy SQL Server Integration Packages on an Azure-SSIS integration runtime, or for on-premises SQL Server?
	X SQL Server Data Tools
	dtexec
	SQL Server Management Studio

SSIS packages defining a workflow of tasks to be executed. The workflow of tasks in a package is referred to as which of the following?
	SSIS Project
	Data Flow
	X Control flow

When creating a new SSIS Integration Runtime which of the following pieces of information are required on the Deployment setting page? Select all options that apply
	X Admin Username
	X Catalog database Server Endpoint
	Create Catalog (SSISDB)
	X Subscription

$ When migrating SSIS packages. you need to consider the location of the SSIS packages that you are migrating, as this can impact how you migrate the packages. Which of the following storage types are available when migrating SSIS packages? Select all options that apply.
	X MSDB Database in SQL Server
	X File system
	X SSIS Catalog (SSISDB)
	Azure Key Vault
	X SSIS Package Store

You want to ingest data from a SQL Server database hosted on an on-premises Windows Server. What integration runtime is required for Azure Data Factory to ingest data from the on-premises server?
	Azure-SSIS Integration Runtime
	Self-Hosted Integration Runtime
	Azure Integration Runtime

Azure Data Factory allows you to configure a Git repository with either Azure Repos or GitHub.
Which of the following are advantages of using GIT Integration? Select all options that apply.
	PowerShell or an SDK changes are published directly to GIT
	X Better CI/CD
	X Source control
	X Partial saves

In Azure Data factory what is a supported connector for built-in parameterization?
	Azure Data Lake Storage Gen2
	Azure Key Vault
	X Azure Synapse Analytics

What effect does removing a Git configuration from a Data Factory have on the repository?
	It deletes everything from the repository. The factory will still contain all published resources and you can continue to edit the factory directly against the service.
	X It does not delete anything from the repository and the factory will still contain all published resources and you can continue to edit the factory directly against the service.
	It deletes everything from the repository. The factory will also lose access to all published resources, and you cannot continue to edit the factory directly against the service.

Which of the following would you select when you need to merge changes in your feature Branch to your collaboration branch?
	X Create pull request
	Create Merge Request
	Create Push Request

By default, the Azure Data Factory user interface experience (UX) authors directly against the data factory service. Which of the following are true in respect of the Azure Data Factory user interface experience? Select all options that apply.
	The Data Factory service is optimized for collaboration and version control.
	X The Data Factory service is not optimized for collaboration and version control.
	X Does not include a repository for storing the JSON entities for changes.
	Includes a repository for storing the JSON entities for changes.

Using the Azure Data factory user interface, which of the following can be used to connect a Git repository to your data factory for both Azure Repos and GitHub? Select all options that apply.
	X ADF Authoring Hub
	X ADF Management hub
	X During factory creation
	X ADF Home Page
	ADF Monitor Hub

Each Azure Repos Git repository that's associated with a data factory has a collaboration branch with a default name of main. Users can also create feature branches. Which of the following statements are true?
	You can publish to the Data Factory service from your Feature branch only.
	You can publish to the Data Factory service from both Collaboration and Feature Branches.
	X You can publish to the Data Factory service from your collaboration branch only.

When implementing Git integration with your Azure Data Factory you typically don't want every team member to have permissions to update the Data Factory. To publish to the Data Factory a user must have which of the following permissions?
	XData Factory contributor
	User Access Administrator
	Managed Application Operator Role

If your ADF development factory has an associated git repository, you can override the default Resource Manager template parameters of the Resource Manager template generated by publishing or exporting the template. However, there is a limit on the number of allowed parameters in a template. Which limit applies to Resource Manager template parameters?
	512
	1024
	X256
	125

Azure Data Factory can integrate with which version control software?
	XGit repositories
	Team Foundation Server
	Source Safe

Which feature in alerts can be used to determine how an alert is fired?
	Add severity
	Add rule
	XAdd criteria

Azure Data Factory provides support for both code-free Extract Transform Load (ETL) and Extract Load Transform (ELT) processes.
	False
	X True

When creating an Azure Data Share from within the portal you must specify which of the following? Select all options that apply.
	X Name
	Version
	X Location
	X Resource Group Name
	Tag

Azure Data Share provides open and flexible data sharing, including the ability to share from and to different data stores. Which of the following Data stores support incremental snapshots? Select all that apply
	X Azure Data Lake Storage Gen2
	X Azure Data Lake Storage Gen1
	Azure SQL Database
	X Azure Blob Storage

When creating Azure Data Shares which of the following should be selected to enable access to data at source?
	Snapshot sharing
	X In-Place sharing

A Mapping Data Flow is a visually designed transformation service in Azure Data Factory. With the ability to visually design things such as different types of joins, aggregate functions etc. Which of the following Join types are supported in Mapping Data Flow? Select all options that apply.
	X Right Outer
	Union
	X Left Outer
	X Inner Join
	X Full Outer

In Azure Data factory after you have finished joining and transforming a dataset through a mapping data flow, it is important to write the newly created dataset into a destination store. Which of the following options would you use to do this?
	Union
	X Sink
	Merge
	Copy

To add or remove datasets created with Azure Data Share which of the following is True?
	X It is possible to add or remove datasets within Azure Data Share after it has been created.
	It is only possible to remove or add datasets before it's sent within Azure Data Share.
	It is not possible to add or remove datasets if created with Azure Data Share.

What must be done when a connector in Azure Data Factory is not supported in a mapping data flow task to transform data from one of these sources?
	X Ingest the data into a supported source using the copy activity.
	Use an aggregate transformation in the Dataflow activity.
	Use a group by activity in the Dataflow activity.

You want to add a data flow activity to a pipeline, which of the following options would you choose?
	X Move and Transform
	General
	Azure Data Explorer
	Batch Service

Which transformation can be used to load data into a data store or compute resource?
	X Sink
	Source
	Window

Microsoft Azure provides a variety of data platform services that enables you to perform different types of analytics. Predictive analytics can be implemented through which of the following features? Select all options that apply.
	Azure Data Lake Storage Gen2
	X Machine Learning Services
	X HDInsight
	X Azure Databricks

Azure Data Factory integration runtime enables bridging between activities and linked Services objects. Which of the following are types of Integration Runtime in Azure data factory? Select all options that apply.
	X Self Hosted
	Azure Spark
	Azure-SQL
	X Azure-SSIS
	X Azure

In Azure Data Factory to get a view of the history of debug runs, or see all the active debug runs, you can navigate to the monitor tab. Azure Data Factory service keeps Debug run history for how many days?
	5 Days
	X 15 days
	365 Days
	30 Days

You need to deploy and manage existing SSIS packages using Azure Data Factory. Which of the following tools would you use? Select all options that apply
	X SQL Server Data Tools (SSDT)
	X? SQL Server Management Studio (SSMS) 
	Azure Copy Activity
	Azure Data Factory Pipelines

Pipelines in Azure Data Factory typically perform four distinct steps. Identify these steps? Select all options that apply.
	X Monitor
	Data Analysis
	XTransform and Enrich
	X Publish
	X Connect and Collect

When setting up Azure Data factory through the Azure Portal. Which of the following pieces of Information are required? Select all options that apply.
	X Subscription
	X? Instance Name
	Management Group Name
	X Resource Group name
	X Region

To create Data Factory instances, the user account that you use to sign into Azure must be a member of which of the following roles? Select all options that apply.
	X The Owner role
	Monitoring Contributor
	Managed Application Operator Role
	X The Contributor Role

In Azure Data factory there are many activities that are possible in a pipeline. These activities are grouped into 3 categories. Which of the following are categories of activities? Select all options that apply.
	X Data transformation activities
	X Data movement activities
	X Control activities
	Branching Activities

Much of the functionality of Azure Data Factory appears in Azure Synapse Analytics as a feature called pipelines. You can use it to integrate data pipelines between which of the following?
	X SQL Serverless
	X SQL Pools
	X Spark Pools
	Apache Hive

SSIS packages define a workflow of tasks to be executed. The workflow of tasks in a package is referred to as which of the following?
	SSIS Project
	X Control flow
	Data Flow

## C4

Which one of the following Azure solutions provides an integrated analytics platform that combines data warehousing, big data analytics, data integration, and visualization into a single environment?
	Azure Data Lake Storage Gen2
	Azure Databricks
	Azure Data Factory
	X Azure Synapse Analytics

You have unplanned or ad-hoc workloads to process within Azure Synapse Analytics. Which of the following offers is best suited to assist with this task?
	Azure Synapse serverless SQL pools
	Azure Synapse dedicated SQL pools
	Azure Synapse Pipelines

Which of the following languages are supported in Apache Spark within Azure Synapse Analytics? Select the three languages that apply.
	C#
	Scala
	JSON
	PySpark

An Azure Synapse Analytics workspace stores data in which of the following?
	Azure SQL Database
	X Apache Spark Tables
	Cosmos DB Tables
	Azure Data Lake Storage Gen2

You need to create and enable SQL or Spark pools. Can you perform these tasks within the Azure Synapse Analytics workspace?
	X Yes
	No

Do you need to create a Data Lake Gen2 storage account before creating an Azure Synapse Analytics workspace?
	X No
	Yes

In a data warehouse data is retrieved, cleansed, and transformed from a range of source data system, and is then served in a structured relational format. What is this format commonly referred to as?
	Fact table
	Dimension Table
	Snowflake Schema
	X Star Schema

You create a dedicated SQL pool to ensure predictable performance and cost by reserving processing power for data permanently stored in SQL tables. Which of the following services are you making use of through this process?
	X Data warehousing
	Data virtualization

You are working in Azure Synapse Analytics, and you have a requirement to ingest data from a data source in readiness to prepare the data for transformation and analysis. Which feature should you implement to enable the ingestion of the data from a data source?
	X Linked Service
	SQL Pool
	Spark pool
	Datasets

You need to create a pipeline in Azure Synapse Studio. Which hub should you navigate to?
	Develop hub
	X Integrate hub
	Manage hub
	Data hub

You need to access the Power BI Workspace from within Azure Synapse Studio. Which hub should you navigate to?
	X Develop Hub
	Integrate Hub
	Manage Hub
	Data Hub

You perform analytics over a database system that is seen to provide transactional capabilities without impacting the performance of the system. What is this process referred to as?
	Data Warehousing
	Data Visualisation
	X Hybrid Transactional and Analytical processing (HTAP)
	Data Virtualisation

You need to run near real-time analytics over operational data in Azure Cosmos DB without impacting the performance of transactional workloads on Azure Cosmos DB. Can you perform this task using Azure Synapse Link for Cosmos DB?
	X Yes
	No

Which Azure Service is Azure Synapse Pipelines based on?
	Azure Data Explorer
	Azure Stream Analytics
	X Azure Data Factory

Which three of the following Azure Data Lake file types can be explored and analyzed directly with Synapse Analytics SQL and Spark?
	XML
	X JSON
	X CSV
	X Parquet

You are creating a new Azure Data Lake Storage Generation 2 workspace. In which instance do you need to ensure that the name of the workspace is unique?
	Within the selected Resource Group only.
	Within the selected Subscription only.
	X Globally within Microsoft Azure.

A data warehouse provides which one of the following features?
	A repository of data distributed across multiple instances of Azure SQL.
	X A central repository of data stored in relational tables.
	A central repository of data stored in a nonrelational database.

The ability to interact with data without the need to understand its type, format, or structure is referred to as…
	X Data Virtualization
	Data Visualization
	Data Ingestion
	Data Warehousing

What method can you use to populate permanent tables with data in a data warehouse?
	X Extract, Transform and Load (ETL)
	Extract, Load and Transform (ELT)

Which of these actions does a driver node perform when working with Spark pool clusters?
	X The Driver node sends work to the Worker nodes and instructs them to pull data from a specified data source.
	The Driver node pulls data from a specified data source and sends the work to the Worker nodes.

Which three of the following features in Azure Synapse Analytics can you use to create a new Spark pool?
	X Synapse Analytics .Net SDK
	X Azure Portal
	JSON
	X Azure PowerShell

Which one of the following components of Azure Synapse Analytics can the different engines use to share the databases and tables between Spark pools and SQL on-demand engine?
	Azure Synapse Link
	X Azure Synapse shared metadata
	Azure Synapse Spark pools

Which one of the following Azure Synapse Analytics components can you use to perform Hybrid Transaction and Analytical processing?
	Azure Synapse Studio
	Azure Synapse Pipeline
	X Azure Synapse Link

Where in Azure Synapse Analytics can you develop TSQL scripts and notebooks?
	X Azure Synapse Studio
	Azure Data Lake
	Azure portal

Which hub in Azure Synapse Studio is used to view the status of the various integration runtimes that are running, or Apache Spark jobs and data flow debug activities?
	X Monitor Hub
	Develop Hub
	Integrate Hub
	Manage Hub

You are executing a notebook. Which of the following should you select as its compute target?
	X Spark Pool
	Dedicated Pool
	Serverless Pool
	SQL Pool

Which of the following would you use to author data flows in Azure Synapse Studio Develop hub?
	X Code-free GUI
	PySpark
	Parquet
	Scala
You are tasked with creating a Notebook. Which Azure Synapse Studio hub should you access?
	Integrate hub
	Manage hub
	X Develop hub
	Data hub

You want to view the contents of the primary data lake store. Where can you access this content within Azure Synapse Studio?
	X In the linked tab of the Data hub.
	In the workspace tab of the Data hub.
	In the Integration section of the Monitor hub.

You need to ingest data code-free within Azure Synapse Analytics. Which of the following tools can you use to perform this?
	X Azure Data Factory
	Azure Databricks
	Power BI

You need to grant access to the Azure Synapse workspace and its resources. Which hub should you use?
	Integrate hub
	X Manage hub
	Monitor hub

You need to view the contents of the primary data lake. Which hub within Azure Synapse Studio can you use?
	The Integration section of the Monitor hub.
	X The linked tab of the Data hub.
	The workspace tab of the Data hub.

You have recently deployed Azure Synapse Analytics and need to ingest data code-free. Which of the following tools can be used to perform this task?
	Power BI
	Azure Databricks
	X Azure Data Factory

Which of the following features does Power BI support? Select all options that apply.
	X Support for multiple data sources
	X Rich visualizations 
	X Natural Language Query
	Notebooks

Which of the following data formats are natively supported by Azure Synapse Analytics?
	X Parquet
	X CSV
	Scala
	X JSON
	X ORC

Messages can be collected and processed from streaming services using which of the following features?
	X Azure Stream Analytics
	Azure IOT Central
	X Azure Databricks
	X Azure Functions

$ Which of the following is a Big Data Solution that stores data in a relational table format with columnar storage?
	Azure Synapse Spark pools
	X Azure Synapse SQL pool

Which component enables you to perform code free transformations in Azure Synapse Analytics?
	Synapse Copy activity
	Synapse Studio
	X Synapse Mapping data flow
Which transformation in the Mapping Data Flow is used to routes data rows to different streams based on matching conditions?
	X Conditional Split
	GetMetadata activity
	Lookup

Which transformation is used to load data into a destination data store or compute resource?
	Source
	X Sink
	Window

Can the file size, number of files, and folder structure of data stored in Data Lake Storage Gen2 impact on performance?
	No
	X Yes

What is the recommended file size for Data Lake Storage Gen 2?
	256MB to 1GB
	10GB to 100GB
	X 256MB to 100GB
	1GB to 10GB

Enabling Debug mode while building data flows in Azure Synapse turns on which of the following?
	X Spark cluster
	Serverless cluster
	Dedicated SQL Pool
You have recently deployed Azure Synapse Analytics and need to ingest data code-free. Which of the following tools can be used to perform this task?
	X Azure Data Factory
	Power BI
	Azure Databricks

Identify three reasons as to why you might need to add a staging area into the architecture of a modern data warehouse.
	X Enable the ingestion of source systems based on different schedules.
	X To join data from different source systems.
	X To reduce contention on source systems.
	To make data analytical available directly from the staging area.

You are implementing a modern data warehouse which requires a staging area. Which one of the following technologies would be most suited to creating this staging area?
	X Azure Data Lake
	Azure Synapse SQL Pools
	Azure Synapse Spark Pools.

You enable Debug mode while building Data Flows in Azure Synapse Analytics. Which one of the following features does this automatically create?
	A dedicated SQL Pool.
	X A small Spark cluster.
	A serverless cluster.

Which of the following offerings in Synapse Analytics is suitable for unplanned or ad-hoc workloads?
	Azure Synapse dedicated SQL pools
	Azure Synapse Pipelines
	X Azure Synapse serverless SQL pools

Which of the following can be used to develop SQL Scripts and Notebooks?
	X Azure Synapse Studio
	Azure portal
	Data Lake

Creating dedicated SQL pools to reserve processing power for data permanently stored in SQL tables in order to ensure predictable performance and cost is a type of model applied to which of the following?
	X Data warehousing
	Data virtualization

You can create a new Spark pool in Azure Synapse using which three of the following features?
	JSON
	X Azure Portal
	X Azure PowerShell
	X Synapse Analytics .Net SDK

You are working in Azure Synapse Analytics, and you have a requirement to ingest data from a data source in readiness to prepare the data for transformation and analysis. Which feature should you implement to enable the ingestion of the data from a data source?
	SQL Pool
	X Linked Service
	Spark pool
	Datasets

What term do we use to refer to the ability to perform analytics over a database system that is seen to provide transactional capabilities without impacting the performance of the system itself?
	Data Visualization
	Data Virtualization
	Data Warehousing
	X Hybrid Transactional and Analytical processing (HTAP)

Which Azure Synapse Studio hub would you go to create Notebooks?
	Manage hub
	X Develop hub
	Integrate hub
	Data hub

Which three of the following scenarios would be a valid reason for adding a staging area into the architecture of a modern data warehouse?
	To make data analytical available directly from the staging area.
	X To enable the ingestion of source systems based on different schedules.
	X To join data from different source systems.
	X To reduce contention on source systems.

When building data flows in Azure Synapse you enable debug mode. What does debug mode activate?
	A dedicated SQL Pool
	X A Spark cluster
	A serverless cluster

Which of the following Azure solutions provides an integrated analytics platform that combines data warehousing, big data analytics, data integration, and visualization into a single environment?
	Azure Data Factory
	X Azure Synapse Analytics
	Azure Databricks
	Azure Data Lake Storage Gen2
You need to answer the question of “What is likely to happen in the future based on previous trends and patterns?”. Which one of the following types of analytics should you use?
	Diagnostic
	X Predictive
	Prescriptive
	Descriptive

A Spark Pool Cluster is created from which two of the following components?
	X Multiple Worker Nodes
	Multiple Spark drivers
	X A single Spark Driver
	A single worker node

Which Azure Service is Azure Synapse Pipelines based on?
	X Azure Data Factory
	Azure Stream Analytics
	Azure Data Explorer

Which Azure Synapse Studio hub would you go to create Notebooks?
	Integrate hub
	X Develop hub
	Data hub
	Manage hub

You have recently deployed Azure Synapse Analytics. You now need to ingest data code-free. Which of the following tools can be used to perform this task?
	Power BI
	Azure Databricks
	X Azure Data Factory

When building data flows in Azure Synapse you enable debug mode. What does debug mode activate?
	A dedicated SQL Pool
	X A Spark cluster
	A serverless cluster

A data warehouse provides which of the following?
	A repository of data distributed across multiple instances of Azure SQL.
	A central repository of data stored in a nonrelational database.
	X A central repository of data stored in relational tables.

Which four of the following data formats are natively supported by Synapse Analytics when ingesting raw data in batch from new data sources? Choose all that apply
	Scala
	X ORC
	X JSON
	X Parquet
	X CSV

Processing data that arrives in real-time /near real-time is also referred to as streaming data processing. Azure offers purpose-built stream ingestion services such as Azure IoT Hub and Azure Event Hubs. To collect messages from these or similar services, and process them, you can use which of the following features?
	X Azure Databricks
	Azure IoT Central
	X Azure Functions
	X Azure Stream Analytics

Which technology is typically used as a staging area in a modern data warehousing architecture?
	Azure Synapse Spark Pools.
	Azure Synapse SQL Pools
	X Azure Data Lake
True or False
When data is stored in Data Lake Storage Gen2, the file size, number of files, and folder structure can have an impact on performance.
	False
	X True

When working with Data Lake Storage Gen2 many small files can negatively affect performance. The recommended file size for Data Lake Storage Gen2 is between which of the following sizes?
	1GB to 10GB
	X 256MB to 100GB
	10GB to 100GB
	256MB to 1GB

When building data flows in Azure Synapse you can enable debug mode., When Debug mode is enabled Synapse automatically turns on which of the following?
	Dedicated SQL Pool
	X Spark cluster
	Serverless cluster

## C5

A Star schema is a modeling approach widely adopted by relational data warehouses. It requires modelers to classify their model tables as either dimension or fact. Which of the following are features of dimension tables? Select all options that apply.
	X A dimension table contains a key column (or columns)
	X A dimension table describes business entities
	A dimension stores numeric measure columns

Which of the following are true in respect of fact tables? Select all options that apply.
	X A fact table contains numeric measure columns
	A fact table describes business entities
	X Fact tables store observations or events
	X A fact table contains dimension key columns that relate to dimension tables.

Since Synapse Analytics is a massively parallel processing (MPP) system, you need to consider how data is distributed in your table design. What is the recommended distribution option for Fact tables?
	Clustered Columnstore Index
	Clustered Index
	X Hash-distribution
	Replicate

Which of the following statements are true in respect of a Star schema?
Select all options that apply.
	X Star schemas have a high level of Data redundancy.
	In a Star schema Cube processing might be slow because of the complex join.
	X Star schema dimension tables are a denormalized data structure.
	X A Star schema contains a fact table surrounded by dimension tables.
	A Star schema will have a fact table surrounded by dimension tables which are in turn surrounded by dimension tables.

Examine the following statement and select the missing word with an entry from those supplied below:
A time dimension table is one of the most consistently used dimension tables. This type of table enables consistent _____________ for temporal analysis and reporting
	X granularity
	uniqueness
	indexing
	distribution

What distribution option would be best for a sales fact table that will contain billions of records?
	DISTRIBUTION = REPLICATE
	DISTRIBUTION = HEAP
	X DISTRIBUTION = HASH([SalesOrderNumber])

Examine the following statement and replace the missing words with your choice from the supplied options.
A fact table contains dimension key columns that relate to dimension tables. Dimension key columns determine the ________ of a fact table, while the dimension key values determine the _________ of a fact table.
	X dimensionality, granularity
	granularity, dimensionality

What type of system is Azure Synapse Analytics?
	Online Transactional Processing (OLTP) Database
	Symmetric Multiprocessing (SMP) system
	X Massively Parallel Processing (MPP) system

True or False
A Snowflake Schema is an extension of a Star Schema, and it adds additional dimensions. The dimension tables are normalized which splits data into additional tables.
	X True
	False

To create a snowflake schema in Azure Synapse Studio you navigate to which of the following hubs?
	Manage
	Develop
	X Data
	Integrate

What distribution option would you use for a product dimension table that will contain 1,000 records in Synapse Analytics?
	DISTRIBUTION = ROUND_ROBIN.
	DISTRIBUTION = HASH([ProductId]).
	X DISTRIBUTION = REPLICATE.

What is the difference between a star schema and a snowflake schema?
	A star schema uses surrogate keys while a snowflake schema uses business keys.
	All dimensions in a star schema are normalized while all dimensions in a snowflake schema join directly to the fact table (denormalized).
	A star schema has one fact table while a snowflake schema has multiple fact tables.
	X All dimensions in a star schema join directly to the fact table (denormalized) while some dimension tables in a snowflake schema are normalized.

Which of the following are true in respect of fact tables?
Select all options that apply.
	A fact table describes business entities.
	X Fact tables store observations or events.
	X A fact table contains numeric measure columns.
	X A fact table contains dimension key columns that relate to dimension tables.

Which of the following statements are true in respect of a Star schema? Select all options that apply.
	X A Star schema contains a fact table surrounded by dimension tables.
	X Star schema dimension tables are a denormalized data structure.
	A Star schema will have a fact table surrounded by dimension tables which are in turn surrounded by dimension tables.
	In a Star schema Cube processing might be slow because of the complex join.
	X Star schemas have a high level of Data redundancy.

What distribution option would be best for a sales fact table that will contain billions of records?
	DISTRIBUTION = REPLICATE
	DISTRIBUTION = HEAP
	X DISTRIBUTION = HASH([SalesOrderNumber])

A Star schema is a modeling approach widely adopted by relational data warehouses. It requires modelers to classify their model tables as either dimension or fact. Which of the following are features of dimension tables? Select all options that apply.
	X A dimension table contains a key column (or columns)
	X A dimension table describes business entities
	A dimension stores numeric measure columns

Which Workload Management capability manages minimum and maximum resource allocations during peak periods?
	Workload Containment
	X Workload Isolation
	Workload Importance

Select from the options below to complete the missing text in the following statement. A data warehouse that is built on a Massively Parallel Processing (MPP) system are built for processing and analyzing large datasets. As such they perform well with ____________ that can be distributed across compute nodes and storage. Select from the options to complete the missing text.
	Multiple small batch sizes
	X Fewer and larger batch sizes

Resource classes are pre-determined resource limits in Synapse SQL pool that govern compute resources and concurrency for query execution. Resource classes can help you configure resources for your queries by setting limits on the number of queries that run concurrently and, on the compute-resources assigned to each query. Which of the following statements are correct? Select all options that apply.
	X Smaller resource classes reduce the maximum memory per query but increase concurrency.
	Larger resource classes increase concurrency, but reduce the maximum memory per query,
	X Larger resource classes increase the maximum memory per query but reduce concurrency.
	Smaller resource classes reduce the concurrency but increase maximum memory per query.

SQL Pools have the concept of concurrency slots, which manage the allocation of memory to connected users. To optimize the load execution operations, you should consider which of the following? Select all options that apply.
	Increase the number of simultaneous load jobs that are running.
	X Assigning higher resource classes that reduce the number of active running tasks.
	Assigning lower resource classes that reduce the number of active running tasks.
	X Reducing or minimizing the number of simultaneous load jobs that are running.

In Synapse SQL pools workload importance influences the order in which a request gets access to resources. There are five levels of importance. Which of the following are valid levels of importance? Select all options that apply.
	X above_normal
	very high
	X below_normal
	X low
	X normal
	X high

How does splitting source files help maintain good performance when loading into Synapse Analytics?
	X Compute node to storage segment alignment
	Optimized processing of smaller file sizes
	Reduced possibility of data corruptions

Azure Synapse Analytics dedicated SQL pools can support a maximum of how many compute nodes?
	80
	X 60
	40
	20

Select from the options below to complete the missing text in the following statement. A data warehouse that is built on a Massively Parallel Processing (MPP) system are built for processing and analyzing large datasets. As such they perform well with ____________ that can be distributed across compute nodes and storage. Select from the options to complete the missing text.
	X Fewer and larger batch sizes
	Multiple small batch sizes

In Synapse Analytics resource classes are pre-determined resource limits in Synapse SQL pools that govern compute resources and concurrency for query execution. Dynamic Resource Classes allocate a variable amount of memory depending on the service level. Which of the following are valid Dynamic Resource Classes?
	X xlargerc
	xxlargerc
	X largerc
	tinyrc
	X mediumrc
	X smallrc

In Azure Synapse Analytics configured with a dedicated SQL pool, classifiers assign incoming requests to a workload group. Classifiers are evaluated with every request submitted. If a request is not matched to a classifier, it is assigned to the default workload group. The default workload group in is the ___________ resource class.
	mediumrc
	X smallrc
	xlargerc
	largerc

SQL Pools have the concept of concurrency slots, which manage the allocation of memory to connected users. Which of the following should you consider to optimize the load execution operations? Select all options that apply.
	X Reducing or minimizing the number of simultaneous load jobs that are running.
	Assigning lower resource classes that reduce the number of active running tasks.
	X Assigning higher resource classes that reduce the number of active running tasks.
	Increase the number of simultaneous load jobs that are running.

Dedicated SQL pool workload management in Azure Synapse consists of three high-level concepts which give you more control over how your workload utilizes system resources. Which of the following are valid concepts?
	X Workload Classification
	Workload partitioning
	X Workload Importance
	X Workload Isolation

Which of the following hubs would you use to create a workload classifier in Azure Synapse Studio?
	Manage
	Integrate
	Data
	X Develop
Which T-SQL Statement loads data directly from Azure Storage?
	LOAD DATA
	INSERT FROM FILE
	X COPY

Azure Synapse Analytics is a high performing Massively Parallel Processing (MPP) engine that is built with loading and querying large datasets in mind. You have received calls from users reporting that the data in the reports they are producing appears to be out of date. Which of the following is the most likely to cause of out-of-date information being presented in user reports?
	Poor query performance
	Low concurrency
	X Poor load performance

What are the three main table distributions available in Synapse Analytics SQL Pools called. Select all options that apply.
	X Hash distribution
	X Round robin distribution
	X Replicated tables
	Block Distribution

When a table is created, by default the data structure has no indexes and is called a heap
	False
	X True

Select from the following options to complete the missing word in the sentence. Dedicated SQL Pools create a '\_\_\_\_\_\_\_\_\_\_\_\_'  index when no index options are specified on a table
	Non-clustered
	Clustered
	X Clustered columnstore

Materialized views are prewritten queries with joins and filters whose definition is saved and the results persisted to pools. Which of the following pools are the results for Materialized views persisted to?
	Serverless SQL Pool
	X Dedicated SQL Pool

In Azure Synapse SQL you should enable result-set caching when you expect results from queries to return the same values. This option stores a copy of the result set on the control node so that queries do not need to pull data from the storage subsystem or compute nodes. By default, data within the result-set cache is expired and purged by the dedicated SQL pool after how many hours of not being accessed?
	12 Hours
	X 48 Hours
	24 Hours
	36 Hours

$ When data is loaded into Synapse Analytics dedicated SQL pools, the datasets are broken up and dispersed among the compute nodes for processing, and then written to a decoupled and scalable storage layer. What term is used to describe this action?
	Separating
	- Sharing
	X Sharding
	- Shredding

Which of the following table distributions available in Synapse Analytics SQL Pools creates a hash function to deterministically assign each row to a distribution and has a column designated as the distribution column?
	Round robin distribution
	X Hash distribution
	Replicated tables

Which of the following indexing options are available in Dedicated SQL Pools? Select all options that apply.
	Table Index
	X Non-clustered index
	X Clustered index
	X Clustered columnstore index

$ Which of the following index types can be defined on a table or view with a clustered index or on a heap?
	Clustered columnstore
	Clustered
	X Non-clustered

Materialized views are prewritten queries with joins and filters whose definition is saved and the results persisted to pools. Which of the following pools are the results for Materialized views persisted to?
	Serverless SQL Pool
	X Dedicated SQL Pool

You can use Materialized Views to improve the performance of either complex or slow queries. As the data in the underlying base tables change, the data in the materialized view will automatically update without user interaction. However, there are certain restrictions you must comply with when defining a materialized view. Which of the following are valid when defining materialized views?
	X Clustered Columnstore index is supported by materialized views.
	X The hash table distribution is supported in the definition.
	X The round_robin table distribution is supported in the definition.
	  Clustered index is supported by materialized views.
	  The Replicated tables distribution is supported in the definition.

In Azure Synapse SQL, you should enable result-set caching when you expect results from queries to return the same values. This option stores a copy of the result set on the control node so that queries do not need to pull data from the storage subsystem or compute nodes. By default, data within the result-set cache is expired and purged by the dedicated SQL pool after how many hours of not being accessed?
	24 Hours
	36 Hours
	X 48 Hours
	12 Hours

Which Index Type offers the highest compression?
	Rowstore
	Heap
	X Columnstore

How do column statistics improve query performance?
	- By caching column values for queries
	X By keeping track of how much data exists between ranges in columns.
	- By keeping track of the columns which are being queried.

The interoperability between Apache Spark and SQL helps you to directly explore and analyze which of the following types of files?
	X JSON
	X CSV
	X TSV
	YAML
	X Parquet

The Azure Synapse Apache Spark pool to Synapse SQL connector is a data source implementation for Apache Spark. Which of the following is used to efficiently transfer data between the Spark cluster and the Synapse SQL instance?
	Azure Data Lake Storage Generation 2 and JSON.
	X Azure Data Lake Storage Generation 2 and PolyBase.
	Azure Data Lake Storage Generation 2 and XML.

True or False. SQL and Apache Spark share the same underlying metadata store.
	X True
	False

To write data to a dedicated SQL Pool, you use the Write API. The Write API creates a table in the dedicated SQL pool. Which of the following is used to load the data into the table that was created?
	JSON
	X Polybase
	Parquet
	ORC

In what language can the Azure Synapse Apache Spark to Synapse SQL connector be used?
	SQL
	Python
	.Net
	X Scala

When is it unnecessary to use import statements for transferring data between a dedicated SQL and Apache Spark pool?
	When using the PySpark connector.
	- Use token-based authentication.
	X When using the integrated notebook experience from Azure Synapse Studio.

The Develop hub in Azure Synapse Studio is an interface you can use for developing a variety of solutions against an Azure Synapse Analytics instance. In this area, you can create which of the following objects? Select all options that apply.
	X Notebooks
	X SQL Scripts
	X Azure Synapse Pipelines
	X Power BI datasets and reports
	Synapse Workspace

Visual Studio 2019 SQL Server Data Tools (SSDT) has which of the following features? Select all options that apply.
	X Create Database projects within dedicated SQL pools
	X integrate with source control systems
	Create Database projects within Serverless SQL pools
	X Native integration with Azure DevOps

Azure Synapse Analytics supports querying both relational and non-relational data using Transact SQL. The Azure Synapse SQL query language supports different features based on the resource model being used. Which of the following T-SQL Statements are supported on both Dedicated and Serverless Pools? Select all options that apply.
	X Transactions
	Cross database queries (No, doesn't work on Dedicated pools)
	INSERT statement (No, doesn't work on Serverless pools)
	X SELECT statement

Examine the following statement and select from the options below to complete the sentence. Synapse dedicated SQL Pools supports JSON format data to be stored using standard _________ table columns
	- CHAR
	X NVARCHAR
	- TEXT
	- VARCHAR

What Transact-SQL function is used to perform a HyperLogLog function?
	- COUNT
	- COUNT_DISTINCT_APPROX
	X APPROX_COUNT_DISTINCT

In Azure Synapse Studio Develop hub you can define Spark Job definitions. Which of the following languages can be used to define job definitions? Select all options that apply.
	X Scala
	  Transact-SQL
	X .NET Spark
	X PySpark

The Azure Synapse Apache Spark to Synapse SQL connector is designed to efficiently transfer data between which of the following?
	X Serverless Apache Spark pools and Dedicated SQL pools in Azure Synapse.
	Dedicated Apache Spark pools and Serverless SQL pools in Azure Synapse.
	Serverless Apache Spark pools and Serverless SQL pools in Azure Synapse.

The Azure Synapse Studio experience provides an integrated notebook experience. Within this notebook experience, you can attach a SQL or Apache Spark pool, and develop and execute transformation pipelines using which of the following?
	SparkSQL
	X Python
	Scala
	JSON

In Azure Synapse Analytics the authentication process between two systems can be seamless. However, there are some prerequisites. Which of the following role memberships are required to successfully authenticate? Select all options that apply.
	The account used needs to be a member of Storage Blob Data Contributor role in the database or SQL pool from which you transfer data to or from.
	X The account used needs to be a member of the Storage Blob Data Contributor role on the default storage account.
	The account used needs to be a member of the db_exporter role on the default storage account
	X The account used needs to be a member of db_exporter role in the database or SQL pool from which you transfer data to or from.

You have a requirement to transfer data to a dedicated SQL pool that is outside of the workspace of Synapse Analytics. To establish and transfer data to a dedicated SQL pool that is outside of the workspace which form of Authentication can be used?
	None of the above
	Azure AD and SQL Authentication
	X SQL Authentication Only
	Azure AD only

When is it unnecessary to use import statements for transferring data between a dedicated SQL and Apache Spark pool?
	X When using the integrated notebook experience from Azure Synapse Studio.
	Use the PySpark connector
	Use token-based authentication.

To write data to a dedicated SQL Pool, you use the Write API. The Write API creates a table in the dedicated SQL pool. Which of the following is used to load the data into the table that was created?
	X Polybase
	JSON
	ORC
	Parquet

Azure Data Studio is a cross-platform tool to connect and query on-premise and cloud data platforms on windows, macOS, and Linux. Synapse Analytics supports using Azure Data Studio for connecting and querying Synapse SQL on which of the following configurations?
	Only dedicated SQL Pool resources
	X Both dedicated and Serverless SQL Pool resources
	Only Serverless SQL Pool resources

Azure Synapse Analytics supports Approximate execution using Hyperlog accuracy to reduce latency when executing queries with large datasets. Approximate execution is used to speed up the execution of queries with a compromise for a small reduction in accuracy. What percentage accuracy of true cardinality on average will the result return when using Approximate execution?
	1%
	4%
	6%
	X 2%
What Transact-SQL function verifies if a piece of text is valid JSON?
	JSON_QUERY
	X ISJSON
	JSON_VALUE

In Azure Synapse Analytics you can scale a Synapse SQL pool through which of the following? Select all options that apply.
	X PowerShell
	 Parquet
	 JSON
	X Azure portal
	X Transact-SQL
	X Azure Synapse Studio

Apache Spark pools for Azure Synapse Analytics uses an Autoscale feature that automatically scales the number of nodes in a cluster instance up and down. Autoscale continuously monitors the Spark instance and collects metrics. Which of the following conditions will trigger Autoscale to scale up? Select all options that apply.
	X Total pending CPU is greater than total free CPU for more than 1 minute.
	Total pending CPU is less than total free CPU for more than 2 minutes
	Total pending memory is less than total free memory for more than 2 minutes.
	X Total pending memory is greater than total free memory for more than 1 minute

Dedicated SQL pool workload management in Azure Synapse consists of three high-level concepts which gives you more control over how your workload utilizes system resources. Which of the following influences the order in which a request gets access to resources?
	Workload Classification
	Workload Isolation
	X Workload Importance

Which ALTER DATABASE statement parameter allows a dedicated SQL pool to scale?
	X MODIFY
	CHANGE
	SCALE

Which Dynamic Management View enables you to view the active connections against a dedicated SQL pool?
	DBCC PDW_SHOWEXECUTIONPLAN
	sys.dm_pdw_dms_workers
	X sys.dm_pdw_exec_requests

Which workload management feature allows workload policies to be applied to requests through assigning resource classes and importance.
	Workload isolation
	X Workload classification
	Workload importance

In a dedicated SQL pool in Azure Synapse Analytics a distributed table appears as a single table with rows spread across multiple distributions. Across how many distributions are rows stored?
	x 60
	20
	80
	40

Examine the following statement and select from the listed options to complete the sentence. A columnstore index scans a table by scanning column segments of individual rowgroups. Maximizing the number of rows in each rowgroup enhances query performance. For best query performance, the goal is to maximize the number of rows per rowgroup in a columnstore index. Columnstore indexes achieve good performance when rowgroups have at least ____________ rows.
	1,000,000 rows
	1,048,576 rows
	X 100,000 rows
	10,000 rows

SQL pool in Azure Synapse supports standard and materialized views. Which of the following are features of Materialized views? Select all options that apply.
	View content is generated each time the view is used.
	X Speed to retrieve view data from complex queries is Fast.
	Speed to retrieve view data from complex queries is Slow.
	X View content is pre-processed and stored in SQL pool during view creation. The view is updated as data is added to the underlying tables.

What would be the best approach to investigate if the data at hand is unevenly allocated across all distributions?
	Grouping the data based on partitions and counting rows with a T-SQL query.
	Monitor query speeds by testing the same query for each partition.
	X Using DBCC PDW_SHOWSPACEUSED to see the number of table rows that are stored in each of the 60 distributions.

To achieve improved query performance, which of the following would be the best data type for storing data that contains less than 128 characters?
	X VARCHAR(128)
	VARCHAR(MAX)
	NVARCHAR(128)

Which of the following statements is a benefit of materialized views?
	X Reducing the execution time for complex queries with JOINs and aggregate functions.
	Increased high availability
	Increased resiliency benefits