## W2. Design and implement a star schema

### Design and implement data stores in a modern data warehouse

##### Design and implement a star schema
Star schema is a mature modeling approach widely adopted by relational data warehouses
A star schema is highly denormalized so that the fact table joins directly to dimensions
Requires modeles to classify their model tables as either dimension or fact
	Dimension tables describe business entities
		Entities: People, places, concepts
		Normally contain a small number of rows
		TimeDimension table: Is normally included
		Id column: Contains a key column or columns that act as a unique identifier
	Fact table store observations or events
		Examples: sales orders,stock balances, exchange rates...
		Normally contain a large number of rows and keep growing over time
		Contains:
			Key columns
				Relate to dimension tables
				Determines the dimensionality of a fact table				
			Numeric measure columns: 
				Determines the granularity of a fact table

##### Design and implement a snowflake schema
A snowflake schema is a set of normalized tables for a single business entity.
A snowflake schema normalizes some dimensions into multiple tables.
It is a variation of the star schema
There are more dimension tables that relate to one another.
Data distribution in Azure Synapse:
	Ref: https://rajanieshkaushikk.com/2020/09/09/how-to-choose-right-data-distribution-strategy-for-azure-synapse/
	Options:
		Replicated
			Smaill dimensions tables in a star schema
			Will result in a copy of the table on each compute node, which performs well with joins to the distributed fact table
		Round Robin (default)
		Hash
			Large fact tables or large dimension tables

## W3 Data loading and querying in Azure Synapse Analytics

### Data loading and querying in Azure Synapse Analytics

Key design goals in Data Warehousing:
	Optimizing and sppeding up data loads to minimize the impact on the performance of ongoing queries
Lots of tools to load data into SQL Pools:
	Directly from Azure Storage with Transact-SQL and the copy statement
		Load data from Blob Storage
		Load data from Data Lake
	Pipelines data flows (Integrate hub)
	Polybase by defining external tables
		Use T-SQL to access files located on Azure Storage like they were in your SQL pool
Strategies for managing source data files:
	Maintaining a well engineered Data Lake structure
		For instance: Folder hierarchies
		In general having well defined zones for data storage in the data lake
	Compress and optimize files
		Good for large datasets
		Less time spent in data transfers
		Once the compress file is receive you can use the MPP capabilities to extract and load data
		Supported import formats:
			Resource Configuration
			GZip
			Parquet
			Optimized Row Columnar
	Splitting source files
		Azurer Synapse Analytics dedicated SQL pools is segmented into 60 storage segments and a maximum of 60 MPP compute nodes
To make use of the MMP (massive parallel processing) capabilities try to use batch type loads and updated. Those can be distributed across the compute nodes and storage.
Dynamic resource classes:
	Database roles:
		Smallrc - Between 3% and 25% of the resources
		Mediumrc
		largerc
		xlargerc
	When you scale up to a larger service level your queries automatically get more memory
	Currency Slots:
		A feature of SQL pools which manage the allocation of memory to connected users
		By assigning a higher resource class such as largerc you can reduce the number of active running tasks
Don't use Administror account for loading data -> Limmited to smallrc dynamic resource class
It is better to crate specific accounts assigned to different resource classes dependent on the anticipated task
Workload Management
	Manage resource availability when workloads are competing
	For faster load times create a workload classifier for the load user with the important set to high
	Three concepts for workload management:
		Workload Classification
			The simplest classification is load and query
				Load: Insert, update and delete
				Query: Select
			You can also subclassify the queries workload:
				Cube refreshes
				Dashboard queries
				Ad hoc queries
			Not all SQL statements are classified: Begin, commit, rollback...
		Workload Importance
			Higher importance can be used to ensure key sales data is loaded before weather data or a social data feed
			Five levels:
				Low
				below normal
				normal (default)
				above normal
				high.
		Workload Isolation
			Reserves resources for a workload group
			Workload group allow you to define the amount of resources that are assigned per request.

### Optimize data warehouse query performance in Azure Synapse Analytics

Azure Synapse Analytics is aMPP (Massive Parallel Processing) engine
Symptoms that indicate performance issues:
	Poor query performance
	Poor load performance
	Low concurrency
Steps to optimize
	First ensure that the data warehouse is set to the appropriate service level range
	To address the low concurrency issue you can scale the service 
Synapse Analytics SQL Pools.
	To understand MPP it is important to know about table geometries:
	Geometry
		The way data is distributed in available compute nodes.
		Three types:
			Round Robin distributed table (Default)
				Distributes data evenly across all nodes
				A node is chosen randomly and then the data is distributed sequentially.
				Fast performance as stagging table
				Fast for loading data, not so good for querying it, particularly with joins.
			Hash
				Use a hash to assign each row to one distribution deterministically
				In the table one of the column is designated as the 'hash' or distribution column
				The distribution function uses the values of that column to assign the value to a particular distribution
				Best performance for joins and aggregations in large tables.
				Normally better query performance than Round Robin, particularly for large datasets.
				Frequent data modifications will cause the cached copy to be invalidated and requiere the table to be recached.
				Scaling the SQL pool will also require the table to be recached.
			Replicated
				Catches a full copy on each compute node
				Fastest query performance for small tables
				Extra storage is required
		When data is received it is broken to be distributed in the nodes.
		Then it is written to a decoupled and scalable storage layer.
	Indexes:
		When a table is created data structure has no indexes and it is called a heap.
		A well designed indexing system can consume less resources
			Specially when using filtering scans and joins
		Dedicated SQL Pools have the following indexing options:
			Clustered columnstore (default)
				Highest level of data compression
				Best overall query performance
				Best choice for large tables. Outperforms rowstore or heap tables
				Columnstore archive -> Option to gain more compression
				Works on segments of 1024000 rows that get compressed and optimized by columns
			Clustered
				Defined how the table is stored ordered by the columns used for the index
				there can only be one clustered index on a table
				good for queries and joints that require ranges of data to be scanned in the same order that the index is defined.
			Non clustered
				each index row contains the non clustered key value and a row locator
				Data structure separate or additional to the table or heap
				You can create multiple non clustered indexes on a table.
				Best when combining columns and a join group by
				Best then clauses return an exact match or few rows.
	Materialized views:
		Materialized views are pre-written queries with joints and filters whose definition is saved and the results persisted to a dedicated SQL pool
		Not supported in serverless SQL pools
		You can use it to improve the performance of either complex or slow queries
		Restrictions:
			SELECT must meet one of two criteria
				Contain an aggregation function: 
					Max, min, average, count, count_big, sum, etc
			GROUP BY is used and all columns in group by are included in the SELECT
				Up to 32 columns can be used
			Only hash and robin table distribution is supported
			Only supports a clustered column store
		Transactions 
			Are supported
			Adhere to ACID
				Atomicity
				Consistency
				Isolation
				Durability
			Locking and blocking mechanisms maintain transactional integrity
			Isolation levels: 
				Read uncommitted
				Can be changed to Read committed
		Result-set caching
			Enable it when you expect results from the queries to return the same values.
			This stores a copy of the results set on the control node.
			Capacity of the results cash = 1Tbyte
			It is purged after 48 hours
			
## W4 Integrating pools and developer tools in Azure Synapse Analytics

### Integrate SQL and Apache Spark pools in Azure Synapse Analytics

You can create both:
	SQL pools 
	Spark pools
Azure Synapse Analytics makes it easy for you to switch from one to another.
You can use booth to analyze Parquet, csv, tsv, json
Fast data transfer between SQL pools and Apache Spark Pools
The integration works because of:
	Azure Synapse Apache Spark to Synapse SQL connector
		Transfer data between serverless Apache Spark pools and dedicated SQL pools
		It works on dedicated SQP pools only
	JDBC API (Java database connectivity)
		Before. 
			Serial transfer of data with JDBC. 
			That is a bottleneck between the two distributed systems
		Now.
			JDBC and Polybase work together
Benefits of the integration
	Make use of big computational power that Apache Spark offers
	Flexibility of using both in one platform
	The integration is seamless and does not require set up
	They share the same metadata stores to transfer data easily
	When doing ETL (basically with SQL) you can call Apache Spark to make use of its power.
	Integrated notebook experiences
Authenticate in Azure Synapse Analytics
	How SQL and Apache Spark Pools authenticate with Azure Synapse Analytics
	If Azure AD auth is configured:
		They use the token service. Therefore there is no need to use credentials if Azure AD auth is configure. 
	If it is not:
		 SQL authentication can be specified
		 This only works with Scala
		 The account needs to be a member of :
			 db_exporter in the SQL pool
			 Storage Blob Data Contributor role on the default storage account.
	You can allow other users to use the connector
		You have to be Blob Storage owner
		The user should have:
			Access to ASynapse worplace
			Permission to run the notebooks.
		Levels of permissions:
			RWX
		ACL - Access control lists

### Understand data warehouse developer features of Azure Synapse Analytics

Window function
	Mathematica equation on a set of data that is defined within a Window
	Instead of applying the aggregation to all rows it applies it to the rows defined by the window.
	Support:
		Cume_dist
		first_value
		lag
		last_value
		lead
		percentile_cont
		percentile_disc
		percent_rank
	OVER Clause
		It is one of the key components of a Window Function
		You can use over to calculate 
			moving averages,
			cumulative aggregates
			running totals
			top end per group
	ROW or RANGE clauses
		Further limit the rows within the partition
		It can be either physical or logical association
		Physical association is achieved by using the rows clause
Approximate execution
	Good for data exploration
	Faster results but not accurate
Stored Procedures
	Stored procedures encapsulate one or more 
		SQL statements
		Reference to a Microsoft.NET framework common language runtime or CLR method
	You can create tables with CETAS (Create External Tables as Select)
	Benefits of encapsulating T-SQL
		Reduces Network traffic
		Provides Security Boundary
		Eases Maintenance
		Improves performance
	The commands are executed as a single batch of code
	Multiple users and client programs can perform operations on underlying database objects through a proceduire even if the users and programs do not have direct permission on thouse underlying objects.
	Procedured are compiled the first time they are executed and the subsequent execution plan is held in cash. As a result it takes less time to process the procedure.

## W5 Manage, optimize and secure a data warehouse

### Manage and monitor data warehouse activities in Azure Synapse Analytics

You can scale up/down your computer resources from
	Azure Portal
	Azure Synapse Studio
Another option is to use auto-scale
	Options:
		When you create the pool
		From Azure Synapse Studio
	When the system will autoscale up:
		Total pending CPU is greater than total free CPU for more than 1 minute
		Total pending memory is greater than total free memory for mmore than 1 minute
	When the system will autoscale down:
		Total pending CPU is less than total free CPU for more than 2 minutes
		Total pending memory is less than total free memory for more than 2 minutes.
Pause compute in Azure Synapse Analytics
	To reduce cost
	Options:
		From the dedicated SQL pool
		From Azure Synapse Studio
Workloads
	Classification
		Simplests classifications
			Load
				Applies to INSERT, UPDATE, DELETE
			Query
				Applies to SELECT
		An advanced classification would subclssify load and query
		Some statements are not classified:
			DBCC commands, BEGIN, COMMIT, ROLLBACK TRANSACTION
	Importance
		5 levels:
			low
			below_normal
			normal
			above_normal
			high
	Isolation
		Reserves resources for a workload group	
Azure Advisor
	It is free
	Recommendations on the following areas:
		Cost
		Security
		Reliability
		Operational excellence
		Performance
	It is based on the telemetry data generated by Azure Synapse Analytics
	Recommendations checked every 24 hourse
Dynamic Management Views
	Programmatic experience for monitoring the Azure Synapse Analytics SQL pool activity by using the Transact-SQL language
	The provided views are also used by other services such as Azure Advisor
	90 Dynamic Management Views that can are queried against dedicated SQL pools

### Analyze and optimize data warehouse storage in Azure Synapse Analytics
Understand skewed data and space usage
	Data skew is an over-represented value
	A quick check for skew: DBCC_PWD_SHOWSPACEUSED
A row group can have a maximum of 1048576 rows
Column store indexes achieve good performance when row groups have at least 100.000 rows
Materialized views:
	Queries that involve complex computations including Joins and aggregate functions can benefit from deployed Materialized views.
Minimally logged operations
	Can improve performance
	Keep track of extent allocations and metadata changes only
	Logs the minimal log to rollback the transaction after a failure or request
	Generates less amount of data. More input/output efficient
	The following operations can be minimally logged:
		CREATE TABLE AS SELECT
		INSERT...SELECT
		CREATE INDEX
		ALTER INDEX REBUILD
		DROP INDEX
		TRUNCATE TABLE
		DROP TABLE ALTER TABLE SWITCH PARTITION
	