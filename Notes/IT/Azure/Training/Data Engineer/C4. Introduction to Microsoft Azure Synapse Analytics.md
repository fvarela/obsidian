W1 - What is Microsoft Azure Synapse Analytics
	What is Azure Synapse Analytics
		Azure Synapse Analytics is an integrated analytics platform
		Combines the following into a single environment:
			Visualization
			Data warehousing
			Data integration
			Bid Data analytics
		Analytics types it supports:
			Descriptive analytics - what is happening
				Uses dedicated SQL pool
				Use the serverless SQL pool to create a data warehouse interactively
			Diagnostic analytics - why is happening
			Predictive analytics - what is likely to happen in the future
				Uses integrated Apache Spark engine
			Prescriptive analytics - what should I do
				Based on real time or near real-time analysis of data
	How Azure Synapse Analytics works
		Features
			Azure Synapse SQL
			Apache Spark Pool
			Azure Synapse Pipelines
				Leverages the capabilities of Azure Data Factory
				Offers cloud based ETL
			Azure Synapse Link
			Azure Synapse Analytics
			Azure Data Platform
W2 - Components of Azure Synapse Analytics
	Create Azure Synapse analytics workspace
		The first step to deploy Azure Synapse Analytics :deploy Azure Synapse analytics workspace.
		This creates several resources including:
			Data Lake Storage Generation 2 with container
		The workspace stores data in Apache Spark tables
		For predictable costs: Dedicated SQL pools
		For short workloads: Serverless SQL endpoint
			Good for data exploration
			Good for data virtualization
		It also stores application logs.
		Endpoints
			Can be used to connect to the SQL on-demand service and the Azure  Synapse Analytics workspace itself
			Can be
				SQL pools
				Spark pools
	Describe Azure Synapse Analytics SQL
		With Azure Synapse Pipelines you can achieve the same thing than with ADF
		Data virtualization
			Allows you to explore the data without knowing the specifications of the data
			Unlocks insights from data without a warehouse
	Explain Apache Spark in Azure Synapse Analytics
		Apache Spark is an open source distributed service that is use in analycing big data workloads.
		Apache Spark loads data in memory
		Spark pool clusters:
			Group of computers that act as a single computer
			Consists of Spark driver + Worker nodes.
			Benefits:
				Ease of creation
				Ease of use
				Scalability
			Start time 2 minutes for <60 nodes
			Start time 5 minutes for >60 nodes
			End time 5 minutes after last execution unlees kept on by a notebook connection
			'AutoScale' enables add or remove nodes as needed.
			Spark pools can be shut down with no loss of data: all the data is stored in Azure Storage
			Can use:
				Data lake gen2
				Blob Storage
			Primary use:
				Handle big data workloads that cannot be handled by Azure Synapse SQL
			Anaconda preinstalled
W3 - Build An Analytical Solution
	Orchestrate data integration with Azure Synapse pipelines
		Pipelines allows you to build ETL and ELT with dataflows or with compute services such as:
			Azure Databricks
			Azure Synapse Analytics
			Azure HDInsight
		Use:
			SQL Pools
			Spark pools
			SQL serverless
		'Linked service' 
			Connect data sources.
			Prepares the data for transformation or analysis
			Fire compute services on demand
		Datasets
			Represent data structures within the data store
			Can be use by ADF 'Activities'
		Activities
			Contains the tranformation logic
			Can be grouped in a pipeline.
			Pipelines are based on Azure Data Factory
			Triggered by time or event
			Examples
				Copy activity
				Mapping data flow
				Transform via script executions
				Analyzing with machine learning
		Integration runtime
			Bridge activities and linked services
			Three types:
				Azure
				Self hosted
				ADF supports in addition Azure SSIS
		Azyure synapse studio
			Allows HTAP with Cosmos DB
				HTAP is Hybrid Transactional and Analytical processing
				HTAP allows analytics over operational data without impacting the performance of transactional workloads.
			Hubs:
				Develop
					Connect to power bi
					SQL scripts
					Notebooks
					Data flows
					Spark jobs
				Integrate
					Pipelines
				Monitor
					View of performance
				Manage
					Create configure and resume SQL pools and Apache Spark pools
					Create and delete Linked services
					Create and execute triggers
					Manage security access
				Data
					Houses all workspace databases
					Allows you to link to external datasets			
		Azure Synapse Analytics SQL can be explored directly from the Data Lake:
			Json
			CSV
			Parket
W4 - Explore Azure Synapse Studio
	Use Azure Synapse Studio
		It organizes in hubs:
			Home hub
				Ingest, explore, analyze, visualize data
			Data hub
				Access provisioned SQL pool databases
				Access serverless SQL databases
				Access storage accounts, linked services
				Preview data tables
			Develop hub
				Manage SQL scripts
				Synapse notebooks
				Data flows
				Power bi repots
			Integrate hub
				Manage pipelines (same as data factory)
				You don't need to use ADF for data movement and transformation pipelines.
			Monitor hub
				View pipelines
				Check triggers
				Check Integration Runtimes
				Debugging issues
				See history of activities
				SQL requests
			Manage hub
				Managing SQL and Spark pools
				Managing linked services
				Managing integration runtimes
				Create pipeline triggers
				Access control
				Credentials
				Git configuration
	Understand the Azure Synapse Alaytical processes
		Azure Synapse Exclusive
			Works perfect with green field solutions
		Azure Synapse Hybrid
			To integrate it with tools you may already have:
				Azure Data Factory
				Azure Databricks
W4 - Desing a Modern Data Warehouse using Azure Synapse Analytics
	Build a modern data warehouse
		A modern data warehouse allows you to bring together all your data at any scale.
		Scaling with Azure Synapse Analytics is easy.
		It can get info from files that were impossible years ago like pdf, sound
	Define a modern data warehouse architecture
		Tools to achieve data warehouse insights:
			Analytics dashboards
			Operational reporting
			Advanced analytics
		Data warehousing before Azure Synapse Analytics:
			Store in Azure Data Lake Storage gen 2
			Ingest an prepare with ETL or ELT using ADF
			Other option for data preparation:
				Azure Data Bricks
			Then the data is ready for consumption by analytical tools
		Azure Synapse Analytics
			Modern Data Warehouse + cloud scale analytical solution
			Support many data formats natively including:
				CSV
				JSON
				AVRO
				Parquet
				ORC
				TXT
			Features:
				Dedicated SQL pool that leverages massively parallel processing that allows:
					Enterprise data warehousing
					Big data analytics
	Work with data in a modern data warehouse
		Understand data storage for a modern data warehouse
			Staging area
				Although you can ingest data a the source directly into a data warehouse, it is more typical to store data within a staging area
				Also known as Landing Zone
				Sits between source systems and data warehouse
				Reasons to have a Staging area:
					Reduce contention on source systems
						It could be very important not to disrupt the source systems.
						They could handle mission critical functions
						To go easy on the Source Systems you can dump data withouth transformation or cleansing into the staging area
					Deal with the ingestion of source systems on different schedules
					Join data together from different source systems
						Mapping tables: Create new tables by joining data together from different sources
					Rerun failed data warehouse loads from a staging area
				Azure Data Lake is the best suited to be the Staging Area
				A Data Lake is a repository or data that is stored in its natural format, usually as blobs or files
		Understand file formats and structure for a modern data warehouse
			Data loading velocity classification:
				Batch
					Queries that take tens of minutes, hours or days
				Interactive query
					Queries that produce results in seconds to minutes
				Real time query
					Queries of infinite stream of data. Done in millisecond or seconds.
			Supported raw files fortmat that can be ingested by Synapse Analytics in batch:
				CSV
				Parquet
				ORC
				Json
			If you use Apache Spark with Synapse Notebooks you can use more file types
			Streaming data
				Ingest Streaming data into Synapse dedicated SQL pool:
					Azure IoT Hub
					Azure Event Hubs
				Then you can process the streaming data with:
					Azure Stream Analytics
					Azure Functions
					Azure Databricks
				The goal is to save the streaming data to the Data Lake
				Then you analyze the files with Synapse Spark notebooks
				And save the output as csv or other Synapse supported formats
				Simplyfied alternative: Azure Stream Analytics:
					Uses Azure IoT Hub as an input source
					IoT Hub outputs files to the ADLS Gen2 Storage Account
			Techniques to load the data from Data Lake to Dedicated SQL Pool
				Synapse Pipelines
				Synapse Spark Pool
				Copy statements
			Another option. Azure Output
				Combine Azure Stream Analytics with Azure Synapse Analytics to create output sink. This allows you to write the data into a SQL pool without storing it first anywhere else
			Recommended file types
				Raw data - Native format
				Relational data - csv
				web APIs - JSON
				Refined data for querying - parquet
					Parquet is high performance column orientated format optimized for big data
					Has storage and performance benefits
					Compression is efficient
					Easier to query. Fast performance
					Stores file schema in the metadata
			Azure data lake gen 2 features hierarchical namespaces
			In Azure Data Lake gen 2 it is recommended to have separated:
				Production storage account
				Dev and test storage account
			In Azure Data Lake it is better to store few large files than many small files
			