### 1. Configure the secrets
See **Handling Secrets.**
Try to do the credential pass through if you can assign roles in Azure
If that is not possible don't hardcode the passwords. Use the Azure key-vault and access them with dbutil
### 2. Mount the containers using dbfs or abfs

### 3. Ingest Data
See **Ingestion**.
You may want to start with a full ingestion first (simpler)
For production scenarios you would move to an incremental ingestion
Raw data -> Dataframes -> Tables
	Use Pyspark 
Tables can be either managed or external. Check **Tables and Views / Tables - Theory**.

### 4. Query Data
See Querying - Modifying.
Normally you would read from a Table or View