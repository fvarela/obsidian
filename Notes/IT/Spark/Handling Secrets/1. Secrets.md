
Many ways to authenticate:
	* Access Data Lake using AAD Credential Pass-through
	* Entering the Secrets
		* Where - Cluster Scoped Authentication
		* Where - On the Notebooks
		* How - Access Data Lake using Service Principal	
		* How - Access Data Lake using Access Keys
		* How - Access Data Lake using SAS Token
		* Security Note - Secure your secrets!
		

## Access Data Lake using Credential Pass-Through

This allows to access Azure Resources without writing credentials in your code. **This is the best option** but you need to be able to assign roles in order to use it. A contributor cannot do that

Configure this in Databricks
![[Pasted image 20240126092522.png]]


Then you would go to the Azure Storage Account / Access Control (IAM) / Add role assignment.
Then you would look for yourself and give yourself the role of Storage Blob Data Contributor



## Entering the Secrets
### Where - Cluster Scoped  Authentication


Set the configurations you need in the cluster as key value pairs:
![[Pasted image 20240126091746.png]]

You cam also access the secret using the dbutil like this:


That would be equivalent as access your secret from the notbook with:
``` 
formula1dl_access_token = dbutils.secrets.get(scope="vps-vida-keys-scope", key="formula1dl-access-key")

spark.conf.set("fs.azure.account.key.formula1dlfvv.dfs.core.windows.net",

               formula1dl_access_token)
```

### Where - Notebook Scoped Authentication
Just write the same configurations but in the notebooks.


### How - Using Service Principal (Microsoft Entra ID)
1. Register Azure AD Application /Service Principal
2. Generate a Secret / password for the application
3. Set Spark Configuration with App/ Client Id, Directory/ Tenant Id & Secret
4. Assign Role Storage Blob Data Contributor to the Data Lake
### How - Using the Access Keys

`spark.conf.set("fs.azure.account.key.<storage-account>.dfs.core.windows.net"."<access key>")`

then you can list the blobs or whatever:
`dbutils.fs.ls("abfss://demo@formula1dlfvv.dfs.core.windows.net")`

The problem with this approach is that the user has access to the whole storage account. In order to provide access to what is only needed for the project you can use SAS:


### How - Using SAS Token

```
spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfx.core.windows.net", "SAS)
spark.conf.set("fs.azure.sass.token.provider.type.<storage-account>.dfx.core.windows.net","org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider")
spark.conf.set("fs.azure.sas.fixed.token.<storage-account>.dfx.core.windows.net","<token>")
```



### Security Note - Secure your secrets! 
You need to create:
	Azure key-vault
	Databricks backed Secret Scope
And link the two together

* Use Databricks backed Secret Scope
	* To create it apppend `#secrets/createScope` to the Databricks URL:
	`[Databricks (azuredatabricks.net)](https://adb-2323455902314664.4.azuredatabricks.net/?o=2323455902314664#secrets/createScope)`
	* There you can add the information of your Azure key-vault:
		* Retrieved the needed info from Azure key-vault properties
			* Vault URI -> DNS Name
			* Resource ID

* Accessing the secrets with dbutils:

	```
	dbutils.secrets.help()
	dbutils.secrets.listScopes()
	dbutils.secrets.get(scope='formula1-scope', key='formula1-account-key') 
	```




